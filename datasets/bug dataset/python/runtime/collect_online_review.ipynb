import praw

# Your Reddit API credentials (using direct credentials as confirmed working)
client_id = "4a2hdSlOEJsfvC4T83mHEw"
client_secret = "iMmYdQTdvGfyIx3XUrm76GKm9y20mA"
user_agent = "pgurwanbaawgai crawler 1.0" # Replace with your user agent

# Initialize PRAW
try:
    reddit = praw.Reddit(
        client_id=client_id,
        client_secret=client_secret,
        user_agent=user_agent
    )

    # Fetch the submission (post) using the URL
    submission_url = "https://www.reddit.com/r/replit"
    submission = reddit.submission(url=submission_url)

    # Print post details
    print(f"Title: {submission.title}")
    print(f"Score: {submission.score}")
    print(f"Number of comments: {submission.num_comments}")
    print(f"URL: {submission.url}")

    # Fetch all comments by replacing "More Comments"
    print("\nFetching all comments...")
    submission.comments.replace_more(limit=None)
    all_comments = submission.comments.list()

    print(f"Successfully fetched {len(all_comments)} comments.")

    # Perform analysis on comments
    print("\nAnalyzing comments:")

    # Basic analysis: Count comments containing specific keywords
    replit_mentions = sum(1 for comment in all_comments if "replit" in comment.body.lower())
    debugging_mentions = sum(1 for comment in all_comments if "debug" in comment.body.lower())

    print(f"Number of comments mentioning 'replit': {replit_mentions}")
    print(f"Number of comments mentioning 'debug': {debugging_mentions}")

    # You can add more analysis here, e.g., sentiment analysis, topic modeling, etc.

except Exception as e:
    print(f"An error occurred: {e}")
